<h1 align="center">Dockerized ML Model (Iris Dataset) with FastAPI + Streamlit + Docker</h1>

###  Live : https://docker-ml-api-frontend.streamlit.app/

This project demonstrates how to train a machine learning model using scikit-learn, serve predictions via a FastAPI endpoint, and containerize the entire app using Docker integrated with simple streamlit & requests UI for specific iris dataset. This API turns raw machine learning predictions into a simple, accessible service. 
```bash
Imagine you’re handed a flower and asked:
“Can you tell me which type of Iris flower this is?”

This trained machine learning model with iris dataset which contains measurements of 3 types of Iris flowers: Setosa, Versicolor, Virginica says:
“Yes! Just give me the measurements of its petals and sepals.”

This model learned patterns from this data - like:
“Setosa usually has small petals, Virginica has longer ones… & hence give predictions.”
```
##### -> A frontend UI (Streamlit) where users can enter values and see the result = https://docker-ml-api-frontend.streamlit.app/
##### -> A backend API (FastAPI + Docker) that takes measurements and returns a prediction = https://docker-ml-api.onrender.com/docs


It’s a clean, minimal setup perfect for learning how to deploy ML models as APIs with these ideas : 

→ Trains model

   a.) Uses scikit-learn to train a logistic regression model on the Iris dataset.

   b.) Saves the model as model.pkl.
   
→ Serves predictions via FastAPI

   a.) Defines a /predict endpoint that accepts JSON input.

   b.) Loads the trained model and returns predictions based on input features.
   
→ Runs inside Docker

   a.) Uses a Dockerfile to package everything (code + dependencies).

   b.) Makes the API portable and easy to deploy anywhere.
   
→ Accepts requests and returns results

   a.) You can send feature data (like [5.1, 3.5, 1.4, 0.2]) and get a prediction.

   b.) The API responds with the predicted class label.

## How it works :

###  Frontend (Streamlit / Requests)
#### → `streamlit_app.py`
        -> Provides a user interface to input iris features  
        -> Sends data to the FastAPI backend  
        -> Displays predicted species with a friendly message  

#### → `requirements.txt`
        -> Lists frontend dependencies: `streamlit`, `requests`  

#### → `Dockerfile`
        -> Builds the frontend container  
        -> Installs dependencies  
        -> Launches the Streamlit app

###  Backend (FastAPI)
#### → `train_model.py`
        -> Trains your ML model and saves it as a .pkl file
        -> Loads the Iris dataset using scikit-learn
        -> Trains a logistic regression model
        -> Serializes the model using pickle and saves it as model.pkl
        
        Run this once to generate the model before building the Docker image.
#### → `model.pkl`
        -> Stores your trained ML model in binary format
        -> This is the output of train_model.py
        -> Used by app.py to make predictions
        -> Should be kept in sync with your training logic
        
        Don’t edit this manually - it’s generated by Python.
### → `app.py`
        -> Serves your ML model as a REST API using FastAPI
        -> Loads model.pkl
        -> Defines a /PREDICT endpoint that accepts JSON input
        -> Returns predictions as JSON output
        
        This is the brain of API - it connects users to your model.
#### → `requirements.txt`
        -> Lists all Python dependencies needed for your project.
        -> fastapi for the web framework
        -> uvicorn as the ASGI server
        -> scikit-learn for ML training and inference
        
        Docker uses this to install everything inside the container.
#### → `DockerFile`
        -> Defines how to build your Docker image.
        -> Starts from a base Python image
        -> Sets the working directory
        -> Copies your project files
        -> Installs dependencies
        -> Launches the FastAPI app with uvicorn
       
        This file turns your local project into a portable container.

## How to Run Locally :

#### 1. Train the model
        -> python train_model.py
        -> O/p = This creates a serialized model your API will use.
#### 2. Build Docker Image
        -> docker build -t ml-api .
        -> O/P = This reads your Dockerfile, installs dependencies, and packages everything into a container.
#### 3. Run Docker Image 
        -> docker run -p 5000:5000 ml-api
        -> O/P = INFO: Uvicorn running on http://0.0.0.0:5000 (That means your FastAPI app is live inside Docker)
#### 4. Test the API
        -> Invoke-RestMethod -Uri http://localhost:5000/predict `
           -Method POST `
           -ContentType "application/json" `
           -Body '{"features": [5.1, 3.5, 1.4, 0.2]}'
        -> O/P = {"prediction": [0]} (This means your model is working and returning predictions)
#### 5. Installations 
#### → Frontend + Backend
        -> pip install -r requirements.txt
           -> O/P = This install all dependicies inside requirements.txt in frontend & backend 
        -> Docker image 
           -> FROM python:3.10
              -> O/P = Starts with an official Python 3.10 image as the base.
           -> WORKDIR /app
              -> O/P = Sets the working directory inside the container to /app.
        -> COPY . /app
           -> O/P = Copies all files from your local project folder into the container’s /app directory.
        -> RUN pip install --no-cache-dir -r requirements.txt
           -> O/P = Installs your Python dependencies inside the container, just like you would locally.
        -> CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "5000"]
           -> O/P = Tells Docker to run your FastAPI app using uvicorn when the container starts.
#### 6. Summary 
        -> Together, this setup works with frontend & backend 
        -> Installs dependencies
        -> Packages your code
        -> Launches your API in a container
#### 7. Final Run 
```bash
Backend (for api testing only ):
cd backend
python train_model.py
docker build -t ml-api .
docker run -p 5000:5000 ml-api
Test API : 
Invoke-RestMethod -Uri http://localhost:5000/predict `
  -Method POST `
  -ContentType "application/json" `
  -Body '{"features": [5.1, 3.5, 1.4, 0.2]}''

O/P :- 
prediction
----------
{0}

To run full app frontend + backend : 
docker-compose up --build
```
#####  → frontend : [http://localhost:8501](https://docker-ml-api-frontend.streamlit.app/)
#####  → backend  : [http://localhost:5000/docs](https://docker-ml-api.onrender.com/docs)

#### 8. Final DockerFile 
```bash
FROM python:3.10
WORKDIR /app
COPY . /app
RUN pip install --no-cache-dir -r requirements.txt
CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "5000"]
```
#### Summary Table 

    | Component |         Tech            |             Purpose                          |
    |-----------|-------------------------|----------------------------------------------|
    | Backend   | FastAPI + scikit-learn  | Serves ML predictions via REST API           |
    | Frontend  | Streamlit + requests    | UI for entering features and viewing results |
    | Container | Docker + Docker Compose | Portable, full-stack deployment              |
    |-----------|-------------------------|----------------------------------------------|
